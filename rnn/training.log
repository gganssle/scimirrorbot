tput: No value for $TERM and no -T specified
vocab.t7 or data.t7 detected as stale. Re-running preprocessing...	
one-time setup: preprocessing input text file /home/ubuntu/scimirrorbot/dat/training/input.txt...	
loading text file...	
creating vocabulary mapping...	
putting data into tensor...	
saving /home/ubuntu/scimirrorbot/dat/training/vocab.t7	
saving /home/ubuntu/scimirrorbot/dat/training/data.t7	
loading data files...	
cutting off end of data so that the batches/sequences divide evenly	
reshaping tensor...	
data load done. Number of data batches in train: 739, val: 39, test: 0	
vocab size: 156	
creating an lstm with 2 layers	
setting forget gate biases to 1 in LSTM layer 1	
setting forget gate biases to 1 in LSTM layer 2	
number of parameters in the model: 298652	
cloning rnn	
cloning criterion	
1/36950 (epoch 0.001), train_loss = 5.06226445, grad/param norm = 5.6384e-01, time/batch = 0.7644s	
2/36950 (epoch 0.003), train_loss = 4.63807549, grad/param norm = 1.7082e+00, time/batch = 0.6776s	
3/36950 (epoch 0.004), train_loss = 3.84951955, grad/param norm = 1.5944e+00, time/batch = 0.6756s	
4/36950 (epoch 0.005), train_loss = 3.67430020, grad/param norm = 1.4315e+00, time/batch = 0.6711s	
5/36950 (epoch 0.007), train_loss = 3.46376744, grad/param norm = 8.2413e-01, time/batch = 0.6824s	
6/36950 (epoch 0.008), train_loss = 3.54341604, grad/param norm = 9.6020e-01, time/batch = 0.6686s	
7/36950 (epoch 0.009), train_loss = 3.59553554, grad/param norm = 7.1970e-01, time/batch = 0.6759s	
8/36950 (epoch 0.011), train_loss = 3.46430358, grad/param norm = 6.0696e-01, time/batch = 1.6478s	
/home/ubuntu/scimirrorbot/train_cron.sh: line 37: 30368 Killed                  th train.lua -data_dir $TRANDIR -batch_size 10 -gpuid -1 -checkpoint_dir $MODLDIR -savefile $TRNUSR

real	0m25.317s
user	0m6.812s
sys	0m0.304s
